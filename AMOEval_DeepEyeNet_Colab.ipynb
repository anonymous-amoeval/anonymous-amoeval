{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bed5c5fd",
   "metadata": {},
   "source": [
    "# AMOEval — End-to-end Colab Notebook (Template → Filling → Judging → Stats)\n",
    "\n",
    "This notebook runs the full AMOEval pipeline from raw inputs:\n",
    "\n",
    "1) **Template induction** from training reports (LLM reads reports → outputs a fixed multi-level entity template).  \n",
    "2) **Optional report generation** from images (+keywords) using one or more multimodal generators.  \n",
    "3) **Two-step template filling** (select Level-1 categories → fill Level-2/3 entities with evidence spans).  \n",
    "4) **Entity-wise judging** for each doctor/model report pair with a 4-way taxonomy (**Aligned / Mismatched / Omitted / Extra**).  \n",
    "5) **Scope-map analysis** (IS vs OOS) + summary tables exported for the paper.\n",
    "\n",
    "**Anonymity note (MICCAI)**: do not include author names, emails, affiliations, or private dataset links in this notebook or the repository.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6cd139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (Colab)\n",
    "!pip -q install openai pandas tqdm pillow openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9f0d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, re, base64, random, textwrap\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "# IMPORTANT: do NOT hardcode API keys.\n",
    "# In Colab, you can paste your key when prompted, or set it as a secret/environment variable.\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    import getpass\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter OPENAI_API_KEY: \")\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616b6189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 0) USER CONFIGURATION\n",
    "# =========================\n",
    "\n",
    "# Required inputs (only these two are required):\n",
    "IMAGES_ROOT = Path(\"/content/images\")          # folder containing images (e.g., .png/.jpg)\n",
    "REPORT_JSON_PATH = Path(\"/content/reports.json\")  # one JSON file containing reports + image paths (+ optional keywords/splits)\n",
    "\n",
    "# Outputs\n",
    "OUTPUT_DIR = Path(\"/content/amo_eval_outputs\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Input contract (edit to match your evaluation setting)\n",
    "# - \"image_only\": model sees only images\n",
    "# - \"image_keywords\": model sees images + keywords (recommended for DeepEyeNet setting described in the paper)\n",
    "INPUT_CONTRACT = \"image_keywords\"\n",
    "\n",
    "# Fixed evaluator (used for template induction + filling + judging)\n",
    "EVAL_MODEL_TEXT = \"gpt-5.2\"\n",
    "\n",
    "# Optional multimodal generators (used only if you run the generation stage)\n",
    "# (You can also skip generation and provide MODEL_REPORTS_JSONL yourself.)\n",
    "GENERATOR_MODELS = [\"gpt-5.2\", \"gpt-5-mini\", \"o4-mini\"]\n",
    "\n",
    "# Controls (reduce for quick sanity checks; increase for paper-scale runs)\n",
    "RANDOM_SEED = 7\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "MAX_TRAIN_REPORTS_FOR_TEMPLATE = 800   # number of training reports sampled for template induction\n",
    "MAX_TEST_SAMPLES = None               # e.g., 200 for quick runs; None = full test split\n",
    "\n",
    "# File paths\n",
    "TEMPLATE_JSON = OUTPUT_DIR / \"amo_eval_template.json\"\n",
    "MODEL_REPORTS_JSONL = OUTPUT_DIR / \"model_reports.jsonl\"      # generated (or user-provided) model reports\n",
    "DOCTOR_FILLED_JSONL = OUTPUT_DIR / \"doctor_filled.jsonl\"\n",
    "MODEL_FILLED_JSONL = OUTPUT_DIR / \"model_filled.jsonl\"\n",
    "SEMANTIC_CSV = OUTPUT_DIR / \"semantic_sets_full.csv\"\n",
    "SUMMARY_XLSX = OUTPUT_DIR / \"label_stats_summary.xlsx\"\n",
    "\n",
    "# Scope map (editable): prefixes treated as Out-of-Scope (OOS) under the chosen input contract.\n",
    "# Everything else is treated as In-Scope (IS).\n",
    "# NOTE: this is an explicit *user decision* that encodes the input contract.\n",
    "OOS_PREFIXES = [\n",
    "    \"patient.demographics\",\n",
    "    \"patient.history\",\n",
    "    \"ocular.exam.visual_acuity\",\n",
    "    \"treatments\",\n",
    "    \"clinical_course\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d89b9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 1) LOAD DATA\n",
    "# =========================\n",
    "\n",
    "def _first_nonempty(d: Dict[str, Any], keys: List[str]) -> Optional[Any]:\n",
    "    for k in keys:\n",
    "        if k in d and d[k] not in [None, \"\", []]:\n",
    "            return d[k]\n",
    "    return None\n",
    "\n",
    "def load_report_json(report_json_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Load a single JSON file that contains {image path, doctor report, optional keywords, optional split}.\n",
    "    Supports several common schemas to reduce friction when sharing anonymized code.\n",
    "    \"\"\"\n",
    "    with open(report_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    records: List[Dict[str, Any]] = []\n",
    "\n",
    "    def add_item(item: Dict[str, Any], split_override: Optional[str] = None, fallback_id: Optional[str] = None):\n",
    "        img_rel = _first_nonempty(item, [\"image_rel\", \"image_relpath\", \"image_path\", \"image\", \"img\", \"filepath\", \"path\"])\n",
    "        report = _first_nonempty(item, [\"doctor_report\", \"reference_report\", \"report\", \"text\", \"report_text\", \"gt_report\"])\n",
    "        keywords = _first_nonempty(item, [\"keywords\", \"keyword\", \"tags\", \"key_words\"])\n",
    "        split = split_override or _first_nonempty(item, [\"split\", \"set\", \"subset\"]) or \"unknown\"\n",
    "        sid = _first_nonempty(item, [\"sample_id\", \"id\", \"uid\", \"study_id\", \"case_id\"]) or fallback_id\n",
    "\n",
    "        if img_rel is None or report is None:\n",
    "            return  # skip malformed rows silently\n",
    "\n",
    "        records.append({\n",
    "            \"sample_id\": str(sid) if sid is not None else f\"sample_{len(records)}\",\n",
    "            \"split\": str(split).lower(),\n",
    "            \"image_rel\": str(img_rel),\n",
    "            \"keywords\": \"\" if keywords is None else (keywords if isinstance(keywords, str) else \" \".join(map(str, keywords))),\n",
    "            \"doctor_report\": str(report),\n",
    "        })\n",
    "\n",
    "    # Schema A: dict with train/val/test lists\n",
    "    if isinstance(data, dict) and any(k in data for k in [\"train\", \"test\", \"val\", \"valid\", \"validation\", \"dev\"]):\n",
    "        for k in [\"train\", \"val\", \"valid\", \"validation\", \"dev\", \"test\"]:\n",
    "            if k in data and isinstance(data[k], list):\n",
    "                split = \"val\" if k in [\"val\", \"valid\", \"validation\", \"dev\"] else k\n",
    "                for j, item in enumerate(data[k]):\n",
    "                    if isinstance(item, dict):\n",
    "                        add_item(item, split_override=split, fallback_id=f\"{split}_{j}\")\n",
    "    # Schema B: dict with a 'data' list\n",
    "    elif isinstance(data, dict) and \"data\" in data and isinstance(data[\"data\"], list):\n",
    "        for j, item in enumerate(data[\"data\"]):\n",
    "            if isinstance(item, dict):\n",
    "                add_item(item, fallback_id=f\"row_{j}\")\n",
    "    # Schema C: list of dicts\n",
    "    elif isinstance(data, list):\n",
    "        for j, item in enumerate(data):\n",
    "            if isinstance(item, dict):\n",
    "                add_item(item, fallback_id=f\"row_{j}\")\n",
    "    # Schema D: dict mapping id -> dict\n",
    "    elif isinstance(data, dict):\n",
    "        for j, (k, item) in enumerate(data.items()):\n",
    "            if isinstance(item, dict):\n",
    "                item2 = dict(item)\n",
    "                item2.setdefault(\"id\", k)\n",
    "                add_item(item2, fallback_id=str(k))\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported REPORT_JSON schema. Please provide a list/dict of samples.\")\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "    if df.empty:\n",
    "        raise ValueError(\"No valid samples found. Check REPORT_JSON_PATH and field names.\")\n",
    "\n",
    "    # If split is missing for all rows, default everything to 'test'\n",
    "    if df[\"split\"].nunique() == 1 and df[\"split\"].iloc[0] in [\"unknown\", \"none\", \"\"]:\n",
    "        df[\"split\"] = \"test\"\n",
    "\n",
    "    # Canonicalize split names\n",
    "    df[\"split\"] = df[\"split\"].replace({\"validation\":\"val\", \"valid\":\"val\", \"dev\":\"val\"})\n",
    "\n",
    "    return df\n",
    "\n",
    "df = load_report_json(REPORT_JSON_PATH)\n",
    "\n",
    "# If no train split exists, create a deterministic 80/20 split so template induction can run.\n",
    "if \"train\" not in set(df[\"split\"]):\n",
    "    df = df.sample(frac=1.0, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "    cut = int(0.8 * len(df))\n",
    "    df.loc[:cut-1, \"split\"] = \"train\"\n",
    "    df.loc[cut:, \"split\"] = \"test\"\n",
    "    print(\"[WARN] No 'train' split found in REPORT_JSON; created an 80/20 split for template induction vs evaluation.\")\n",
    "\n",
    "# Resolve image paths\n",
    "df[\"image_path\"] = df[\"image_rel\"].apply(lambda p: str(IMAGES_ROOT / p))\n",
    "\n",
    "print(\"Loaded samples:\", len(df))\n",
    "print(df[\"split\"].value_counts())\n",
    "\n",
    "df_train = df[df[\"split\"] == \"train\"].reset_index(drop=True)\n",
    "df_test = df[df[\"split\"] == \"test\"].reset_index(drop=True)\n",
    "\n",
    "if MAX_TEST_SAMPLES is not None:\n",
    "    df_test = df_test.iloc[:MAX_TEST_SAMPLES].copy()\n",
    "\n",
    "df_train.head(2), df_test.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a003f5ee",
   "metadata": {},
   "source": [
    "## Stage I — Template induction (fixed multi-level entity template)\n",
    "\n",
    "The evaluator reads a subset of training reports and produces a fixed slot catalog (entity template).  \n",
    "Later stages must *reuse* this template; no new slots are allowed during filling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccf675b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 2) STAGE I: TEMPLATE INDUCTION\n",
    "# =========================\n",
    "\n",
    "INDUCE_SYSTEM = (\n",
    "    \"You are a medical NLP ontology induction system for ophthalmology clinical narrative reports. \"\n",
    "    \"Your job is to design a fixed, reusable, fine-grained template (slot catalog) that can cover the entire corpus.\"\n",
    ")\n",
    "\n",
    "INDUCE_USER_BASE = \"\"\"You will be given a corpus of ophthalmology clinical-description texts (free-text narratives).\n",
    "\n",
    "Goal:\n",
    "- Induce a FIXED template (slot catalog) that can represent ALL information in the corpus.\n",
    "- Slot set must be stable and reusable across the dataset. Later extraction MUST NOT add new slots.\n",
    "\n",
    "Hard requirements:\n",
    "1) Coverage: any new report from the same dataset should be representable without inventing new fields.\n",
    "2) Stability: the slot set is fixed; missing info is null/unknown, never new slots.\n",
    "3) Fine granularity: prefer atomic facts (symptoms, exam findings, imaging, labs, treatments, response, follow-up timeline, referrals).\n",
    "4) Uncertainty/diagnostic status must be representable: suspected/confirmed/ruled_out/unknown etc.\n",
    "5) Include explicit support for multi-modal tests mentioned in text (e.g., FA/OCT/fundus photo, MRI/CT, LP/labs, etc.), referral chain, and treatment response/adjustments/recurrence.\n",
    "\n",
    "Slot naming conventions:\n",
    "- Use dot-paths (e.g., section.subsection.item), snake_case.\n",
    "- For repeating items, use [] in path if needed (e.g., imaging.studies[].modality).\n",
    "- Provide a few 'other_*' catch-all slots so that truly rare details are still captured without new slots.\n",
    "\n",
    "Output:\n",
    "Return JSON only with:\n",
    "- template_name, template_version\n",
    "- global_conventions: missing_value_policy, status_enum, certainty_enum, evidence_policy\n",
    "- slots: >100 slots, each slot includes: path, type, description, allowed_values (optional), example_phrases (1–2 short)\n",
    "- coverage_notes\n",
    "\"\"\"\n",
    "\n",
    "UPDATE_USER_BASE = \"\"\"You are updating an existing FIXED slot catalog for the same dataset.\n",
    "\n",
    "Rules (append-only):\n",
    "- You may ONLY append NEW slots if strictly necessary to cover the NEW corpus chunk.\n",
    "- Do NOT delete, rename, reorder, or modify existing slots (paths, types, descriptions).\n",
    "- Output the FULL updated template JSON only.\n",
    "\n",
    "Existing template JSON:\n",
    "{existing_template_json}\n",
    "\n",
    "New corpus chunk:\n",
    "{new_corpus}\n",
    "\"\"\"\n",
    "\n",
    "def _strip_fences(s: str) -> str:\n",
    "    s = s.strip()\n",
    "    if s.startswith(\"```\"):\n",
    "        s = re.sub(r\"^```[a-zA-Z]*\\n\", \"\", s).strip()\n",
    "        s = re.sub(r\"```\\s*$\", \"\", s).strip()\n",
    "    return s\n",
    "\n",
    "def call_llm_json(model: str, system: str, user: str, max_output_tokens: int = 8000) -> Dict[str, Any]:\n",
    "    resp = client.responses.create(\n",
    "        model=model,\n",
    "        input=[\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\", \"content\": user},\n",
    "        ],\n",
    "        temperature=0,\n",
    "        top_p=1,\n",
    "        max_output_tokens=max_output_tokens,\n",
    "    )\n",
    "    txt = _strip_fences(resp.output_text)\n",
    "    try:\n",
    "        return json.loads(txt)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"JSON parse failed: {e}\\n\\nRaw output:\\n{txt[:2000]}\")\n",
    "\n",
    "def enforce_append_only(old_t: Dict[str, Any], new_t: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Keep ALL old slots unchanged; only append truly new slots from new_t.\"\"\"\n",
    "    old_slots = old_t.get(\"slots\", [])\n",
    "    new_slots = new_t.get(\"slots\", [])\n",
    "    old_by_path = {s.get(\"path\"): s for s in old_slots if isinstance(s, dict) and s.get(\"path\")}\n",
    "    merged = list(old_by_path.values())\n",
    "\n",
    "    for s in new_slots:\n",
    "        if not isinstance(s, dict):\n",
    "            continue\n",
    "        p = s.get(\"path\")\n",
    "        if not p or p in old_by_path:\n",
    "            continue\n",
    "        merged.append(s)\n",
    "\n",
    "    merged = sorted(merged, key=lambda x: x.get(\"path\", \"\"))\n",
    "\n",
    "    out = dict(old_t)\n",
    "    out[\"slots\"] = merged\n",
    "\n",
    "    # keep global fields from old if present\n",
    "    for k in [\"template_name\", \"template_version\", \"global_conventions\", \"coverage_notes\"]:\n",
    "        if k in old_t:\n",
    "            out[k] = old_t[k]\n",
    "        elif k in new_t:\n",
    "            out[k] = new_t[k]\n",
    "    return out\n",
    "\n",
    "def chunk_texts(texts: List[str], max_chars: int = 120000) -> List[str]:\n",
    "    chunks=[]\n",
    "    cur=[]\n",
    "    cur_len=0\n",
    "    for t in texts:\n",
    "        t = t.strip()\n",
    "        if not t:\n",
    "            continue\n",
    "        add_len = len(t) + 5\n",
    "        if cur and cur_len + add_len > max_chars:\n",
    "            chunks.append(\"\\n\\n---\\n\\n\".join(cur))\n",
    "            cur=[t]\n",
    "            cur_len=len(t)\n",
    "        else:\n",
    "            cur.append(t)\n",
    "            cur_len += add_len\n",
    "    if cur:\n",
    "        chunks.append(\"\\n\\n---\\n\\n\".join(cur))\n",
    "    return chunks\n",
    "\n",
    "def induce_template_from_reports(train_reports: List[str]) -> Dict[str, Any]:\n",
    "    chunks = chunk_texts(train_reports, max_chars=120000)\n",
    "    template = None\n",
    "\n",
    "    for i, corpus_chunk in enumerate(chunks):\n",
    "        if template is None:\n",
    "            user = INDUCE_USER_BASE + \"\\n\\nCorpus:\\n\" + corpus_chunk\n",
    "            template = call_llm_json(EVAL_MODEL_TEXT, INDUCE_SYSTEM, user, max_output_tokens=12000)\n",
    "        else:\n",
    "            user = UPDATE_USER_BASE.format(\n",
    "                existing_template_json=json.dumps(template, ensure_ascii=False),\n",
    "                new_corpus=corpus_chunk\n",
    "            )\n",
    "            updated = call_llm_json(EVAL_MODEL_TEXT, INDUCE_SYSTEM, user, max_output_tokens=12000)\n",
    "            template = enforce_append_only(template, updated)\n",
    "\n",
    "        print(f\"[Template induction] processed chunk {i+1}/{len(chunks)}; slots={len(template.get('slots', []))}\")\n",
    "\n",
    "    # minimal normalization\n",
    "    template.setdefault(\"template_name\", \"AMOEvalTemplate\")\n",
    "    template.setdefault(\"template_version\", \"v1\")\n",
    "    template[\"slots\"] = sorted(template.get(\"slots\", []), key=lambda x: x.get(\"path\", \"\"))\n",
    "\n",
    "    return template\n",
    "\n",
    "# Run / load template\n",
    "if TEMPLATE_JSON.exists():\n",
    "    print(f\"Using existing template: {TEMPLATE_JSON}\")\n",
    "    template = json.loads(TEMPLATE_JSON.read_text(encoding='utf-8'))\n",
    "else:\n",
    "    train_reports = df_train[\"doctor_report\"].dropna().tolist()\n",
    "    random.shuffle(train_reports)\n",
    "    train_reports = train_reports[:MAX_TRAIN_REPORTS_FOR_TEMPLATE]\n",
    "    print(f\"Inducing template from {len(train_reports)} training reports...\")\n",
    "    template = induce_template_from_reports(train_reports)\n",
    "    TEMPLATE_JSON.write_text(json.dumps(template, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "    print(f\"Saved template to {TEMPLATE_JSON}\")\n",
    "\n",
    "print(\"Template slots:\", len(template.get('slots', [])))\n",
    "print(\"Example slot paths:\", [s.get('path') for s in template.get('slots', [])[:10]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e7a619",
   "metadata": {},
   "source": [
    "## Stage II — Optional report generation (images + keywords → model report)\n",
    "\n",
    "If you already have generated reports, you can skip this stage and place your own `model_reports.jsonl` at `MODEL_REPORTS_JSONL`.\n",
    "Each JSONL row must include: `sample_id`, `model_tag`, `report_text` (and optionally `image_rel`, `keywords`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ab913c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 3) STAGE II (OPTIONAL): GENERATE MODEL REPORTS\n",
    "# =========================\n",
    "\n",
    "GEN_SYSTEM = (\n",
    "    \"You are an ophthalmology report generation system. \"\n",
    "    \"If a requested detail is not supported, omit it.\"\n",
    ")\n",
    "\n",
    "GEN_INSTRUCTION = \"\"\"Write a concise ophthalmology clinical report for the given case.\n",
    "- Use the provided image and keywords (if any).\n",
    "- Prefer imaging-grounded findings and clear, clinically oriented language.\n",
    "\"\"\"\n",
    "\n",
    "def image_to_data_url(image_path: str) -> str:\n",
    "    with open(image_path, \"rb\") as f:\n",
    "        b = f.read()\n",
    "    ext = Path(image_path).suffix.lower().lstrip(\".\")\n",
    "    if ext == \"jpg\":\n",
    "        ext = \"jpeg\"\n",
    "    mime = f\"image/{ext if ext else 'png'}\"\n",
    "    return f\"data:{mime};base64,\" + base64.b64encode(b).decode(\"utf-8\")\n",
    "\n",
    "def generate_one_report(model_name: str, image_path: str, keywords: str) -> str:\n",
    "    kw = (keywords or \"\").strip()\n",
    "    user_txt = GEN_INSTRUCTION + (f\"\\n\\nKeywords: {kw}\\n\" if kw else \"\\n\\nKeywords: (none)\\n\")\n",
    "\n",
    "    resp = client.responses.create(\n",
    "        model=model_name,\n",
    "        input=[\n",
    "            {\"role\": \"system\", \"content\": GEN_SYSTEM},\n",
    "            {\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"input_image\", \"image_url\": image_to_data_url(image_path)},\n",
    "                {\"type\": \"input_text\", \"text\": user_txt},\n",
    "            ]},\n",
    "        ],\n",
    "        temperature=0,\n",
    "        top_p=1,\n",
    "        max_output_tokens=800,\n",
    "    )\n",
    "    return resp.output_text.strip()\n",
    "\n",
    "def load_model_reports_jsonl(path: Path) -> List[Dict[str, Any]]:\n",
    "    if not path.exists():\n",
    "        return []\n",
    "    rows=[]\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line=line.strip()\n",
    "            if not line: \n",
    "                continue\n",
    "            rows.append(json.loads(line))\n",
    "    return rows\n",
    "\n",
    "# Generate reports unless an existing JSONL is already present.\n",
    "if MODEL_REPORTS_JSONL.exists():\n",
    "    print(f\"Using existing model reports: {MODEL_REPORTS_JSONL}\")\n",
    "else:\n",
    "    out_rows=[]\n",
    "    for _, row in tqdm(df_test.iterrows(), total=len(df_test)):\n",
    "        sid = row[\"sample_id\"]\n",
    "        img_path = row[\"image_path\"]\n",
    "        kw = row.get(\"keywords\",\"\")\n",
    "        if not Path(img_path).exists():\n",
    "            raise FileNotFoundError(f\"Missing image: {img_path}\")\n",
    "\n",
    "        for m in GENERATOR_MODELS:\n",
    "            txt = generate_one_report(m, img_path, kw)\n",
    "            out_rows.append({\n",
    "                \"sample_id\": sid,\n",
    "                \"model_tag\": m,\n",
    "                \"image_rel\": row[\"image_rel\"],\n",
    "                \"keywords\": kw,\n",
    "                \"report_text\": txt,\n",
    "            })\n",
    "\n",
    "    with open(MODEL_REPORTS_JSONL, \"w\", encoding=\"utf-8\") as f:\n",
    "        for r in out_rows:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "    print(f\"Saved model reports to {MODEL_REPORTS_JSONL}\")\n",
    "\n",
    "# Quick sanity check\n",
    "rows = load_model_reports_jsonl(MODEL_REPORTS_JSONL)\n",
    "print(\"Model report rows:\", len(rows))\n",
    "print(\"Example row keys:\", list(rows[0].keys()) if rows else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511d7b3c",
   "metadata": {},
   "source": [
    "## Stage II — Template filling (two-step extraction)\n",
    "\n",
    "Step (1): the evaluator reads the report + Level-1 category directory, then selects relevant Level-1 categories.  \n",
    "Step (2): the evaluator reads the selected categories' Level-2/3 entities and extracts localized entity descriptions with short verbatim evidence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9262f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 4) STAGE II: TEMPLATE FILLING\n",
    "# =========================\n",
    "\n",
    "FILL_SYSTEM = (\n",
    "    \"You are an information extraction engine. \"\n",
    "    \"You will be given a clinical report and an entity catalog.\"\n",
    ")\n",
    "\n",
    "SELECT_PROMPT = \"\"\"Given the report, select the relevant Level-1 categories from the directory.\n",
    "Return JSON only: {{\"selected_categories\": [ ... ]}}.\n",
    "\n",
    "Level-1 category directory:\n",
    "{category_directory}\n",
    "\n",
    "Report:\n",
    "{report_text}\n",
    "\"\"\"\n",
    "\n",
    "EXTRACT_PROMPT = \"\"\"Extract entity instances from the report using ONLY the provided entity catalog.\n",
    "\n",
    "Rules:\n",
    "- Strict exclusion: if NOT explicitly supported by the text, OMIT it.\n",
    "- Do NOT output unknown/null/none/absent placeholders.\n",
    "- Evidence must be a SHORT verbatim snippet copied from the text.\n",
    "- Output JSON only as: {{\"extractions\": [{{\"entity\": str, \"value\": str, \"evidence\": str}} ... ]}}.\n",
    "\n",
    "Entity catalog (paths + short descriptions):\n",
    "{entity_catalog}\n",
    "\n",
    "Report:\n",
    "{report_text}\n",
    "\"\"\"\n",
    "\n",
    "def build_level1_categories(slots: List[Dict[str, Any]]) -> List[str]:\n",
    "    cats=set()\n",
    "    for s in slots:\n",
    "        p = s.get(\"path\",\"\")\n",
    "        if not p:\n",
    "            continue\n",
    "        cats.add(p.split(\".\")[0])\n",
    "    return sorted(cats)\n",
    "\n",
    "def build_entity_catalog_for_categories(slots: List[Dict[str, Any]], categories: List[str]) -> List[Dict[str, str]]:\n",
    "    cats = set(categories)\n",
    "    out=[]\n",
    "    for s in slots:\n",
    "        p = s.get(\"path\",\"\")\n",
    "        if not p:\n",
    "            continue\n",
    "        if p.split(\".\")[0] in cats:\n",
    "            out.append({\"path\": p, \"description\": s.get(\"description\",\"\")})\n",
    "    return out\n",
    "\n",
    "def select_categories(report_text: str, level1_categories: List[str]) -> List[str]:\n",
    "    directory = \"\\n\".join([f\"- {c}\" for c in level1_categories])\n",
    "    user = SELECT_PROMPT.format(category_directory=directory, report_text=report_text)\n",
    "\n",
    "    obj = call_llm_json(EVAL_MODEL_TEXT, FILL_SYSTEM, user, max_output_tokens=2000)\n",
    "    sel = obj.get(\"selected_categories\", [])\n",
    "    if not isinstance(sel, list):\n",
    "        sel=[]\n",
    "    sel = [str(x) for x in sel if str(x) in set(level1_categories)]\n",
    "    return sel\n",
    "\n",
    "def extract_entities(report_text: str, entity_catalog: List[Dict[str, str]]) -> List[Dict[str, str]]:\n",
    "    # Keep catalog compact to reduce token cost\n",
    "    catalog_lines = []\n",
    "    for e in entity_catalog:\n",
    "        desc = (e.get(\"description\",\"\") or \"\").strip()\n",
    "        desc = desc[:120]  # truncate\n",
    "        catalog_lines.append(f\"- {e['path']}: {desc}\")\n",
    "    user = EXTRACT_PROMPT.format(entity_catalog=\"\\n\".join(catalog_lines), report_text=report_text)\n",
    "\n",
    "    obj = call_llm_json(EVAL_MODEL_TEXT, FILL_SYSTEM, user, max_output_tokens=12000)\n",
    "    exts = obj.get(\"extractions\", [])\n",
    "    if not isinstance(exts, list):\n",
    "        return []\n",
    "\n",
    "    cleaned=[]\n",
    "    for it in exts:\n",
    "        if not isinstance(it, dict):\n",
    "            continue\n",
    "        ent = it.get(\"entity\")\n",
    "        val = it.get(\"value\")\n",
    "        ev  = it.get(\"evidence\")\n",
    "        if not ent or not val:\n",
    "            continue\n",
    "        cleaned.append({\n",
    "            \"entity\": str(ent),\n",
    "            \"value\": str(val),\n",
    "            \"evidence\": \"\" if ev is None else str(ev),\n",
    "        })\n",
    "    return cleaned\n",
    "\n",
    "level1_categories = build_level1_categories(template.get(\"slots\", []))\n",
    "print(\"Level-1 categories:\", level1_categories)\n",
    "\n",
    "def fill_reports_to_jsonl(df_reports: pd.DataFrame, text_col: str, out_path: Path, extra_fields: List[str]) -> None:\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for _, row in tqdm(df_reports.iterrows(), total=len(df_reports)):\n",
    "            report_text = row[text_col]\n",
    "            sel = select_categories(report_text, level1_categories)\n",
    "            catalog = build_entity_catalog_for_categories(template.get(\"slots\", []), sel)\n",
    "            exts = extract_entities(report_text, catalog)\n",
    "\n",
    "            out = {\n",
    "                \"sample_id\": row[\"sample_id\"],\n",
    "                \"selected_categories\": sel,\n",
    "                \"extractions\": exts,\n",
    "            }\n",
    "            for k in extra_fields:\n",
    "                out[k] = row.get(k, \"\")\n",
    "            f.write(json.dumps(out, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "# 4.1 Fill doctor reports (reference)\n",
    "if DOCTOR_FILLED_JSONL.exists():\n",
    "    print(f\"Using existing doctor-filled JSONL: {DOCTOR_FILLED_JSONL}\")\n",
    "else:\n",
    "    tmp = df_test.copy()\n",
    "    tmp[\"report_text\"] = tmp[\"doctor_report\"]\n",
    "    fill_reports_to_jsonl(tmp, text_col=\"report_text\", out_path=DOCTOR_FILLED_JSONL, extra_fields=[\"image_rel\", \"keywords\"])\n",
    "    print(f\"Saved doctor-filled JSONL: {DOCTOR_FILLED_JSONL}\")\n",
    "\n",
    "# 4.2 Fill model reports (generated)\n",
    "def read_jsonl(path: Path) -> List[Dict[str, Any]]:\n",
    "    rows=[]\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line=line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            rows.append(json.loads(line))\n",
    "    return rows\n",
    "\n",
    "if MODEL_FILLED_JSONL.exists():\n",
    "    print(f\"Using existing model-filled JSONL: {MODEL_FILLED_JSONL}\")\n",
    "else:\n",
    "    model_rows = read_jsonl(MODEL_REPORTS_JSONL)\n",
    "    df_m = pd.DataFrame(model_rows)\n",
    "    if df_m.empty:\n",
    "        raise ValueError(\"MODEL_REPORTS_JSONL is empty. Run generation stage or provide your own model reports JSONL.\")\n",
    "    fill_reports_to_jsonl(df_m, text_col=\"report_text\", out_path=MODEL_FILLED_JSONL, extra_fields=[\"model_tag\", \"image_rel\", \"keywords\"])\n",
    "    print(f\"Saved model-filled JSONL: {MODEL_FILLED_JSONL}\")\n",
    "\n",
    "# quick stats\n",
    "doc_rows = read_jsonl(DOCTOR_FILLED_JSONL)\n",
    "mod_rows = read_jsonl(MODEL_FILLED_JSONL)\n",
    "print(\"Doctor filled rows:\", len(doc_rows), \"Model filled rows:\", len(mod_rows))\n",
    "print(\"Example doctor extractions:\", doc_rows[0][\"extractions\"][:2] if doc_rows else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d42f2f3",
   "metadata": {},
   "source": [
    "## Stage III — Pairwise judging (Aligned / Mismatched / Omitted / Extra)\n",
    "\n",
    "We judge each test case by comparing entity sets extracted from the doctor report vs the model report.  \n",
    "The judge performs semantic matching, classifies doctor-side entities into {Aligned/Mismatched/Omitted}, and then marks any remaining unmatched model entities as Extra.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb86f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 5) STAGE III: ENTITY-WISE JUDGING\n",
    "# =========================\n",
    "\n",
    "JUDGE_SYSTEM = (\n",
    "    \"You judge whether each DOCTOR entity is supported by the MODEL report, \"\n",
    "    \"and whether the model is consistent vs conflicting. \"\n",
    "    \"Then identify any MODEL entities not mentioned by the doctor as EXTRA.\"\n",
    ")\n",
    "\n",
    "JUDGE_PROMPT = \"\"\"You will be given:\n",
    "- doctor_entities: a list of entities extracted from the doctor report (each has entity + value)\n",
    "- model_entities: a list of entities extracted from the model report (each has entity + value)\n",
    "\n",
    "Task:\n",
    "1) For EACH doctor entity, find whether the model has semantically corresponding content (even if under a different entity name).\n",
    "   - If consistent: label ALIGNED\n",
    "   - If contradictory on key clinical attributes: label MISMATCHED\n",
    "   - If not covered by model anywhere: label OMITTED\n",
    "2) After matching against all doctor entities, any remaining unmatched model entity must be labeled EXTRA.\n",
    "\n",
    "Be strict: only mark ALIGNED when the model clearly states the same finding/diagnosis.\n",
    "Output JSON only:\n",
    "{\n",
    "  \"doctor_labels\": [\n",
    "     {\"doctor_index\": int, \"label\": \"ALIGNED|MISMATCHED|OMITTED\", \"matched_model_indices\": [int,...], \"note\": str}\n",
    "  ],\n",
    "  \"extra_model_indices\": [int, ...]\n",
    "}\n",
    "doctor_entities:\n",
    "{doctor_entities}\n",
    "\n",
    "model_entities:\n",
    "{model_entities}\n",
    "\"\"\"\n",
    "\n",
    "def judge_one(doctor_extractions: List[Dict[str, str]], model_extractions: List[Dict[str, str]]) -> Dict[str, Any]:\n",
    "    # compact serialization for prompt\n",
    "    d_ser = json.dumps([{\"entity\": x[\"entity\"], \"value\": x[\"value\"]} for x in doctor_extractions], ensure_ascii=False)\n",
    "    m_ser = json.dumps([{\"entity\": x[\"entity\"], \"value\": x[\"value\"]} for x in model_extractions], ensure_ascii=False)\n",
    "    user = JUDGE_PROMPT.format(doctor_entities=d_ser, model_entities=m_ser)\n",
    "    obj = call_llm_json(EVAL_MODEL_TEXT, JUDGE_SYSTEM, user, max_output_tokens=6000)\n",
    "    return obj\n",
    "\n",
    "def build_scope_group(entity_path: str, oos_prefixes: List[str]) -> str:\n",
    "    for p in oos_prefixes:\n",
    "        if entity_path.startswith(p):\n",
    "            return \"OOS\"\n",
    "    return \"IS\"\n",
    "\n",
    "# Load filled JSONLs\n",
    "doctor_rows = read_jsonl(DOCTOR_FILLED_JSONL)\n",
    "model_rows = read_jsonl(MODEL_FILLED_JSONL)\n",
    "\n",
    "# Index doctor by sample_id\n",
    "doctor_by_id = {r[\"sample_id\"]: r for r in doctor_rows}\n",
    "\n",
    "out_rows=[]\n",
    "\n",
    "for mr in tqdm(model_rows, total=len(model_rows)):\n",
    "    sid = mr[\"sample_id\"]\n",
    "    dr = doctor_by_id.get(sid)\n",
    "    if dr is None:\n",
    "        continue\n",
    "\n",
    "    d_ext = dr.get(\"extractions\", [])\n",
    "    m_ext = mr.get(\"extractions\", [])\n",
    "\n",
    "    judged = judge_one(d_ext, m_ext)\n",
    "\n",
    "    # doctor-side labels\n",
    "    doctor_labels = judged.get(\"doctor_labels\", [])\n",
    "    extra_idx = set(judged.get(\"extra_model_indices\", []))\n",
    "\n",
    "    # write per-doctor-entity records\n",
    "    for item in doctor_labels:\n",
    "        di = item.get(\"doctor_index\")\n",
    "        lab = item.get(\"label\")\n",
    "        mm = item.get(\"matched_model_indices\", [])\n",
    "        note = item.get(\"note\",\"\")\n",
    "        if di is None or di < 0 or di >= len(d_ext):\n",
    "            continue\n",
    "        ent = d_ext[di][\"entity\"]\n",
    "        scope = build_scope_group(ent, OOS_PREFIXES)\n",
    "        out_rows.append({\n",
    "            \"sample_id\": sid,\n",
    "            \"model_tag\": mr.get(\"model_tag\",\"\"),\n",
    "            \"side\": \"doctor\",\n",
    "            \"entity\": ent,\n",
    "            \"value\": d_ext[di][\"value\"],\n",
    "            \"label\": lab,\n",
    "            \"scope\": scope,\n",
    "            \"matched_model_indices\": json.dumps(mm, ensure_ascii=False),\n",
    "            \"note\": note,\n",
    "        })\n",
    "\n",
    "    # write model-side extras\n",
    "    for mi in extra_idx:\n",
    "        if mi < 0 or mi >= len(m_ext):\n",
    "            continue\n",
    "        ent = m_ext[mi][\"entity\"]\n",
    "        scope = build_scope_group(ent, OOS_PREFIXES)\n",
    "        out_rows.append({\n",
    "            \"sample_id\": sid,\n",
    "            \"model_tag\": mr.get(\"model_tag\",\"\"),\n",
    "            \"side\": \"model\",\n",
    "            \"entity\": ent,\n",
    "            \"value\": m_ext[mi][\"value\"],\n",
    "            \"label\": \"EXTRA\",\n",
    "            \"scope\": scope,\n",
    "            \"matched_model_indices\": \"[]\",\n",
    "            \"note\": \"\",\n",
    "        })\n",
    "\n",
    "df_sem = pd.DataFrame(out_rows)\n",
    "df_sem.to_csv(SEMANTIC_CSV, index=False)\n",
    "print(f\"Saved semantic CSV: {SEMANTIC_CSV}\")\n",
    "df_sem.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2024e05",
   "metadata": {},
   "source": [
    "## Aggregation — Export compact tables (for paper)\n",
    "\n",
    "This section aggregates the semantic CSV into:\n",
    "\n",
    "- scope composition (IS vs OOS coverage in doctor references and model assertions)  \n",
    "- outcome rates by scope group  \n",
    "- outcome rates by model  \n",
    "- where Omit / Extra come from (prefix decomposition)\n",
    "\n",
    "All tables are saved to `label_stats_summary.xlsx`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e681d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 6) AGGREGATION + TABLE EXPORTS\n",
    "# =========================\n",
    "\n",
    "df_sem = pd.read_csv(SEMANTIC_CSV)\n",
    "\n",
    "def prefix1(p: str) -> str:\n",
    "    return str(p).split(\".\")[0] if isinstance(p, str) else \"\"\n",
    "\n",
    "def prefix2(p: str) -> str:\n",
    "    sp = str(p).split(\".\")\n",
    "    return \".\".join(sp[:2]) if len(sp) >= 2 else sp[0]\n",
    "\n",
    "# 6.1 Scope composition (what fraction of entities are OOS vs IS?)\n",
    "# - Doctor side: reference entities (Aligned/Mismatched/Omitted are all doctor-side)\n",
    "# - Model side: extras (model-side)\n",
    "doctor_side = df_sem[df_sem[\"side\"]==\"doctor\"].copy()\n",
    "model_side  = df_sem[df_sem[\"side\"]==\"model\"].copy()\n",
    "\n",
    "scope_comp_doctor = (doctor_side[\"scope\"].value_counts(dropna=False).rename_axis(\"scope\").reset_index(name=\"count\"))\n",
    "scope_comp_doctor[\"share_%\"] = 100.0 * scope_comp_doctor[\"count\"] / scope_comp_doctor[\"count\"].sum()\n",
    "\n",
    "scope_comp_model_extra = (model_side[\"scope\"].value_counts(dropna=False).rename_axis(\"scope\").reset_index(name=\"count\"))\n",
    "scope_comp_model_extra[\"share_%\"] = 100.0 * scope_comp_model_extra[\"count\"] / scope_comp_model_extra[\"count\"].sum()\n",
    "\n",
    "# 6.2 Outcome rates by scope (doctor-side labels + model-side extras)\n",
    "# For outcomes, we use a unified view:\n",
    "# - doctor-side labels are ALIGNED/MISMATCHED/OMITTED\n",
    "# - model-side labels are EXTRA\n",
    "def outcome_table(df: pd.DataFrame, group_col: str) -> pd.DataFrame:\n",
    "    pivot = (df.pivot_table(index=group_col, columns=\"label\", values=\"entity\", aggfunc=\"count\", fill_value=0))\n",
    "    # ensure columns exist\n",
    "    for c in [\"ALIGNED\",\"MISMATCHED\",\"OMITTED\",\"EXTRA\"]:\n",
    "        if c not in pivot.columns:\n",
    "            pivot[c]=0\n",
    "    pivot = pivot[[\"ALIGNED\",\"MISMATCHED\",\"OMITTED\",\"EXTRA\"]].reset_index()\n",
    "    pivot[\"n\"] = pivot[[\"ALIGNED\",\"MISMATCHED\",\"OMITTED\",\"EXTRA\"]].sum(axis=1)\n",
    "    for c in [\"ALIGNED\",\"MISMATCHED\",\"OMITTED\",\"EXTRA\"]:\n",
    "        pivot[c] = 100.0 * pivot[c] / pivot[\"n\"].replace(0,1)\n",
    "    return pivot\n",
    "\n",
    "# scope group table: need to combine doctor + model extras and group by scope\n",
    "scope_outcome = outcome_table(df_sem, \"scope\")\n",
    "\n",
    "# model table\n",
    "model_outcome = outcome_table(df_sem, \"model_tag\")\n",
    "\n",
    "# 6.3 Where OMIT and EXTRA come from (prefix2 breakdown)\n",
    "omit_rows = doctor_side[doctor_side[\"label\"]==\"OMITTED\"].copy()\n",
    "extra_rows = model_side[model_side[\"label\"]==\"EXTRA\"].copy()\n",
    "\n",
    "omit_rows[\"prefix2\"] = omit_rows[\"entity\"].apply(prefix2)\n",
    "extra_rows[\"prefix2\"] = extra_rows[\"entity\"].apply(prefix2)\n",
    "\n",
    "omit_comp = omit_rows[\"prefix2\"].value_counts().rename_axis(\"prefix2\").reset_index(name=\"count\")\n",
    "omit_comp[\"share_%\"] = 100.0 * omit_comp[\"count\"] / omit_comp[\"count\"].sum()\n",
    "\n",
    "extra_comp = extra_rows[\"prefix2\"].value_counts().rename_axis(\"prefix2\").reset_index(name=\"count\")\n",
    "extra_comp[\"share_%\"] = 100.0 * extra_comp[\"count\"] / extra_comp[\"count\"].sum()\n",
    "\n",
    "# Save to Excel (compact captions belong in paper text, not in tables)\n",
    "with pd.ExcelWriter(SUMMARY_XLSX, engine=\"openpyxl\") as w:\n",
    "    scope_comp_doctor.to_excel(w, sheet_name=\"scope_comp_doctor\", index=False)\n",
    "    scope_comp_model_extra.to_excel(w, sheet_name=\"scope_comp_model_extra\", index=False)\n",
    "    scope_outcome.to_excel(w, sheet_name=\"outcome_by_scope\", index=False)\n",
    "    model_outcome.to_excel(w, sheet_name=\"outcome_by_model\", index=False)\n",
    "    omit_comp.head(50).to_excel(w, sheet_name=\"omit_prefix2_top50\", index=False)\n",
    "    extra_comp.head(50).to_excel(w, sheet_name=\"extra_prefix2_top50\", index=False)\n",
    "\n",
    "print(f\"Saved summary tables to: {SUMMARY_XLSX}\")\n",
    "print(\"Doctor scope composition:\\n\", scope_comp_doctor)\n",
    "print(\"Outcome by scope:\\n\", scope_outcome)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
